{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "5_My. Моделирование языка с помощью Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxjNjpGrpyEJ"
      },
      "source": [
        "# Language modeling using transformer on Hermann Hesse bibliography data (in Russian)\r\n",
        "\r\n",
        "### Based on course [\"Нейронные сети и обработка текста\"](https://stepik.org/course/54098/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH0NvW8BzsGf"
      },
      "source": [
        "pip install pytorch-nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-M24Wn7D-kSq"
      },
      "source": [
        "pip install youtokentome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "189TS3fH2Cv5"
      },
      "source": [
        "## Required libraries, functions and classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:27:53.780539Z",
          "start_time": "2019-11-05T18:27:52.444607Z"
        },
        "id": "3UMrCkoppyEW"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import functional as F\n",
        "from torchnlp.word_to_vector import BPEmb\n",
        "\n",
        "import youtokentome as yttm\n",
        "\n",
        "import random\n",
        "\n",
        "import heapq\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import datetime\n",
        "\n",
        "from traceback import format_exc\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyGXXxapuloL"
      },
      "source": [
        "def init_random_seed(value=0):\r\n",
        "    random.seed(value)\r\n",
        "    np.random.seed(value)\r\n",
        "    torch.manual_seed(value)\r\n",
        "    torch.cuda.manual_seed(value)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "init_random_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbJvwqUvq4Gb"
      },
      "source": [
        "def copy_data_to_device(data, device):\r\n",
        "    if torch.is_tensor(data):\r\n",
        "        return data.to(device)\r\n",
        "    elif isinstance(data, (list, tuple)):\r\n",
        "        return [copy_data_to_device(elem, device) for elem in data]\r\n",
        "    raise ValueError('Invalid data type {}'.format(type(data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj68ZM-IwWiK"
      },
      "source": [
        "def get_params_number(model):\r\n",
        "    return sum(t.numel() for t in model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz6D2ry6q2Ly"
      },
      "source": [
        "def divisors(n):\r\n",
        "    \"\"\"Find all divisors of a number\"\"\"\r\n",
        "    i = 1\r\n",
        "    divisors = []\r\n",
        "    while i <= n**0.5:\r\n",
        "        if (n % i == 0) : \r\n",
        "            if (n / i == i):\r\n",
        "                divisors.append(i)\r\n",
        "            else:\r\n",
        "                divisors.extend([i, n // i])\r\n",
        "        i = i + 1\r\n",
        "    return sorted(divisors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_LSDUa9ufKL"
      },
      "source": [
        "def split_into_chunks(filename, chunk_size=200):\r\n",
        "    with open(filename) as f:\r\n",
        "        full_text = f.read()\r\n",
        "    return [full_text[start:start + chunk_size] for start in range(0, len(full_text), chunk_size // 2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYs7y8NkMDFJ"
      },
      "source": [
        "def ensure_length(txt, out_len, pad_value):\r\n",
        "    if len(txt) < out_len:\r\n",
        "        txt = list(txt) + [pad_value] * (out_len - len(txt))\r\n",
        "    else:\r\n",
        "        txt = txt[:out_len]\r\n",
        "    return txt\r\n",
        "\r\n",
        "class LanguageModelDataset(Dataset):\r\n",
        "    def __init__(self, sample, chunk_length=100, pad_value=0):\r\n",
        "        self.sample = sample\r\n",
        "        self.chunk_length = chunk_length\r\n",
        "        self.pad_value = pad_value\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.sample)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        text = self.sample[item]\r\n",
        "        start_i = random.randint(0, max(0, len(text) - self.chunk_length - 1))\r\n",
        "        chunk = text[start_i : start_i + self.chunk_length + 1]\r\n",
        "\r\n",
        "        seed_part = chunk[:-1]\r\n",
        "        target_part = chunk[1:]\r\n",
        "\r\n",
        "        seed_part = ensure_length(seed_part, self.chunk_length, self.pad_value)\r\n",
        "        target_part = ensure_length(target_part, self.chunk_length, self.pad_value)\r\n",
        "\r\n",
        "        seed_part = np.array(seed_part)\r\n",
        "        target_part = np.array(target_part)\r\n",
        "\r\n",
        "        return seed_part, target_part"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTe72xVdPHil"
      },
      "source": [
        "def make_target_dependency_mask(length):\r\n",
        "    full_mask = torch.ones(length, length)\r\n",
        "    ignore_mask = torch.tril(full_mask) < 1\r\n",
        "    full_mask.masked_fill_(ignore_mask, float('-inf'))\r\n",
        "    full_mask.masked_fill_(~ignore_mask, 0)\r\n",
        "    return full_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phSORpvwZdn4"
      },
      "source": [
        "def make_positional_encoding(max_length, embedding_size):\r\n",
        "    time = np.pi * torch.arange(0, max_length).float()\r\n",
        "    freq_dividers = torch.arange(1, embedding_size // 2 + 1).float()\r\n",
        "    inputs = time[:, None] / freq_dividers[None, :]\r\n",
        "    result = torch.zeros(max_length, embedding_size)\r\n",
        "    result[:, 0::2] = torch.sin(inputs)\r\n",
        "    result[:, 1::2] = torch.cos(inputs)\r\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZBmKyALZ5lq"
      },
      "source": [
        "class LanguageModel(nn.Module):\r\n",
        "    \"\"\" General class. param::backbone - used architecture of NNet \"\"\"\r\n",
        "    def __init__(self, vocab_size, emb_size, backbone, emb_weights=None, freeze=True, emb_dropout=0.0):\r\n",
        "        super().__init__()\r\n",
        "        if emb_weights is not None:\r\n",
        "            self.embeddings = nn.Embedding.from_pretrained(emb_weights, freeze=freeze, padding_idx=0)\r\n",
        "        else:\r\n",
        "            self.embeddings = nn.Embedding(vocab_size, emb_size, padding_idx=0)\r\n",
        "        self.embedding_size = emb_size\r\n",
        "        self.emb_dropout = nn.Dropout(emb_dropout)\r\n",
        "        self.backbone = backbone\r\n",
        "        self.out = nn.Linear(emb_size, vocab_size)\r\n",
        "    \r\n",
        "    def forward(self, seed_tokenized_sample):\r\n",
        "        batch_size, max_in_length = seed_tokenized_sample.shape\r\n",
        "\r\n",
        "        seed_padding_mask = seed_tokenized_sample == 0\r\n",
        "        dependency_mask = make_target_dependency_mask(max_in_length).to(seed_tokenized_sample.device)\r\n",
        "        \r\n",
        "        seed_embs = self.embeddings(seed_tokenized_sample)  # BatchSize x MaxInLen x EmbSize\r\n",
        "        pos_codes = make_positional_encoding(max_in_length, self.embedding_size).unsqueeze(0).to(seed_embs.device) # 1 x MaxInLen x EmbSize\r\n",
        "        seed_embs = seed_embs + pos_codes\r\n",
        "        seed_embs = self.emb_dropout(seed_embs)\r\n",
        "\r\n",
        "        # BatchSize x TargetLen x EmbSize\r\n",
        "        target_features = self.backbone(seed_embs,\r\n",
        "                                        mask=dependency_mask,\r\n",
        "                                        src_key_padding_mask=seed_padding_mask)\r\n",
        "        logits = self.out(target_features)  # BatchSize x TargetLen x VocabSize\r\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ffXvcbUdnOl"
      },
      "source": [
        "def lm_cross_entropy(pred, target):\r\n",
        "    \"\"\"\r\n",
        "    pred - BatchSize x TargetLen x VocabSize\r\n",
        "    target - BatchSize x TargetLen\r\n",
        "    \"\"\"\r\n",
        "    pred_flat = pred.view(-1, pred.shape[-1])  # BatchSize*TargetLen x VocabSize\r\n",
        "    target_flat = target.view(-1)  # BatchSize*TargetLen\r\n",
        "    return F.cross_entropy(pred_flat, target_flat, ignore_index=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYH07KJVd4n0"
      },
      "source": [
        "class BatchFirstTransformerEncoder(nn.Module):\r\n",
        "    def __init__(self, *args, **kwargs):\r\n",
        "        super().__init__()\r\n",
        "        self.impl = nn.TransformerEncoder(*args, **kwargs)\r\n",
        "        self.initialize_weights()\r\n",
        "    \r\n",
        "    def forward(self, src, *args, **kwargs):\r\n",
        "        src = src.transpose(0, 1).contiguous()  # MaxInLen x BatchSize x EmbSize\r\n",
        "        result = self.impl(src, *args, **kwargs)  # TargetLen x BatchSize x EmbSize\r\n",
        "        result = result.transpose(0, 1).contiguous()  # BatchSize x TargetLen x EmbSize\r\n",
        "        return result\r\n",
        "    \r\n",
        "    def initialize_weights(self):\r\n",
        "        for param in self.impl.parameters():\r\n",
        "            if param.dim() > 1:\r\n",
        "                nn.init.xavier_uniform_(param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx44W288jjRP"
      },
      "source": [
        "def train_eval_loop(model, train_dataset, val_dataset, criterion, lr=1e-3, epoch_n=100, batch_size_train=32,\r\n",
        "                    batch_size_val=32, device=None, early_stopping_patience=10, l2_reg_alpha=0, data_loader_ctor=DataLoader,\r\n",
        "                    optimizer_ctor=None, lr_scheduler_ctor=None, dataloader_workers_n=0, draw_loss=False, show_lr=False):\r\n",
        "\r\n",
        "\r\n",
        "    assert len(train_dataset) % batch_size_train == 0, \"len of train_dataset must be divisible by train_batch_size\"\r\n",
        "    assert len(val_dataset) % batch_size_val == 0, \"len of val_dataset must be divisible by val_batch_size\"\r\n",
        "\r\n",
        "    if device is None:\r\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
        "    device = torch.device(device)\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    if optimizer_ctor is None:\r\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\r\n",
        "    else:\r\n",
        "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\r\n",
        "    \r\n",
        "    if lr_scheduler_ctor is not None:\r\n",
        "        lr_scheduler = lr_scheduler_ctor(optimizer)\r\n",
        "    else:\r\n",
        "        lr_scheduler = None\r\n",
        "\r\n",
        "    \r\n",
        "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size_train, num_workers=dataloader_workers_n)\r\n",
        "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size_val, num_workers=dataloader_workers_n)\r\n",
        "\r\n",
        "    best_val_loss = float(\"inf\")\r\n",
        "    best_epoch_i = 0\r\n",
        "    best_model = deepcopy(model)\r\n",
        "    \r\n",
        "    for epoch_i in range(epoch_n):\r\n",
        "        try:\r\n",
        "            epoch_start = datetime.datetime.now()\r\n",
        "            print(f\"Epoch {epoch_i}\")\r\n",
        "\r\n",
        "            model.train()\r\n",
        "            mean_train_loss = 0\r\n",
        "            train_batches_n = 0\r\n",
        "            history = []\r\n",
        "\r\n",
        "            #for batch_i, (batch_x, batch_y) in enumerate(tqdm(train_dataloader)):\r\n",
        "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\r\n",
        "                batch_x = copy_data_to_device(batch_x, device)\r\n",
        "                batch_y = copy_data_to_device(batch_y, device)\r\n",
        "\r\n",
        "                pred = model(batch_x)\r\n",
        "                loss = criterion(pred, batch_y)\r\n",
        "\r\n",
        "                model.zero_grad()\r\n",
        "                loss.backward()\r\n",
        "\r\n",
        "                optimizer.step()\r\n",
        "\r\n",
        "                mean_train_loss += float(loss)\r\n",
        "                train_batches_n += 1\r\n",
        "\r\n",
        "                if draw_loss:\r\n",
        "                    history.append(loss)\r\n",
        "\r\n",
        "            mean_train_loss /= train_batches_n\r\n",
        "            print('{} iterations, {:0.2f} sec'.format(train_batches_n,\r\n",
        "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\r\n",
        "            print('Average value of the train loss function:', mean_train_loss)\r\n",
        "\r\n",
        "            if draw_loss:\r\n",
        "                plt.plot(history, label=\"loss\")\r\n",
        "                plt.legend()\r\n",
        "                plt.show()\r\n",
        "\r\n",
        "            model.eval()\r\n",
        "            mean_val_loss = 0\r\n",
        "            val_batches_n = 0\r\n",
        "\r\n",
        "            with torch.no_grad():\r\n",
        "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\r\n",
        "\r\n",
        "                    batch_x = copy_data_to_device(batch_x, device)\r\n",
        "                    batch_y = copy_data_to_device(batch_y, device)\r\n",
        "\r\n",
        "                    pred = model(batch_x)\r\n",
        "                    loss = criterion(pred, batch_y)\r\n",
        "\r\n",
        "                    mean_val_loss += float(loss)\r\n",
        "                    val_batches_n += 1\r\n",
        "\r\n",
        "            mean_val_loss /= val_batches_n\r\n",
        "            print('Average value of the validation loss function:', mean_val_loss)\r\n",
        "\r\n",
        "            if mean_val_loss < best_val_loss:\r\n",
        "                best_epoch_i = epoch_i\r\n",
        "                best_val_loss = mean_val_loss\r\n",
        "                best_model = deepcopy(model)\r\n",
        "                print('New best model!')\r\n",
        "            elif epoch_i - best_epoch_i > early_stopping_patience:\r\n",
        "                print('The model has not improved over the last {} epochs, stop training'.format(\r\n",
        "                    early_stopping_patience))\r\n",
        "                break\r\n",
        "  \r\n",
        "            if lr_scheduler is not None:\r\n",
        "                if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\r\n",
        "                    lr_scheduler.step(mean_val_loss)\r\n",
        "                elif isinstance(lr_scheduler, torch.optim.lr_scheduler.StepLR):\r\n",
        "                    lr_scheduler.step()\r\n",
        "                    if show_lr:\r\n",
        "                        print(optimizer.param_groups[0]['lr'])\r\n",
        "                else:\r\n",
        "                    lr_scheduler.step()\r\n",
        "\r\n",
        "            print()\r\n",
        "        except KeyboardInterrupt:\r\n",
        "            print('Stopped early by user')\r\n",
        "            break\r\n",
        "        except Exception as ex:\r\n",
        "            print('Error while training: {}\\n{}'.format(ex, format_exc()))\r\n",
        "            break\r\n",
        "\r\n",
        "    return best_val_loss, best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqx_G7G2f2Bd"
      },
      "source": [
        "class GreedyGenerator:\r\n",
        "    def __init__(self, model, tokenizer, device='cuda', eos_token_id=3):\r\n",
        "        self.model = model\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.device = torch.device(device)\r\n",
        "        self.model.to(self.device)\r\n",
        "        self.eos_token_id = eos_token_id\r\n",
        "\r\n",
        "    def __call__(self, seed_text, max_steps_n=40):\r\n",
        "        seed_tokens = self.tokenizer.encode([seed_text])[0]\r\n",
        "\r\n",
        "        for _ in range(max_steps_n):\r\n",
        "            in_batch = torch.tensor(seed_tokens).unsqueeze(0).to(self.device)\r\n",
        "            best_next_token = self.model(in_batch)[0, -1].argmax()\r\n",
        "            if best_next_token == self.eos_token_id:\r\n",
        "                break\r\n",
        "\r\n",
        "            seed_tokens.append(best_next_token)\r\n",
        "\r\n",
        "        return self.tokenizer.decode([seed_tokens])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt4J8aX4-AMJ"
      },
      "source": [
        "class BeamGenerator:\r\n",
        "    def __init__(self, model, tokenizer, device='cuda', eos_token_id=3):\r\n",
        "        self.model = model\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.device = torch.device(device)\r\n",
        "        self.model.to(self.device)\r\n",
        "        self.eos_token_id = eos_token_id\r\n",
        "\r\n",
        "    def __call__(self, seed_text, max_steps_n=40, return_hypotheses_n=5, beamsize=5):\r\n",
        "        seed_tokens = self.tokenizer.encode([seed_text])[0]\r\n",
        "        initial_length = len(seed_tokens)\r\n",
        "\r\n",
        "        partial_hypotheses = [(0, seed_tokens)]\r\n",
        "        final_hypotheses = []\r\n",
        "\r\n",
        "        while len(partial_hypotheses) > 0:\r\n",
        "            cur_partial_score, cur_partial_hypothesis = heapq.heappop(partial_hypotheses)\r\n",
        "\r\n",
        "            in_batch = torch.tensor(cur_partial_hypothesis).unsqueeze(0).to(self.device)\r\n",
        "            next_tokens_logits = self.model(in_batch)[0, -1]\r\n",
        "            next_tokens_logproba = F.log_softmax(next_tokens_logits, dim=0)\r\n",
        "            topk_continuations = next_tokens_logproba.topk(beamsize)\r\n",
        "\r\n",
        "            for token_score, token_idx in zip(topk_continuations.values, topk_continuations.indices):\r\n",
        "                token_score = float(token_score)\r\n",
        "                token_idx = int(token_idx)\r\n",
        "\r\n",
        "                old_denorm_score = cur_partial_score * np.sqrt(len(cur_partial_hypothesis))\r\n",
        "                new_score = (old_denorm_score - token_score) / np.sqrt(len(cur_partial_hypothesis) + 1)\r\n",
        "\r\n",
        "                new_hypothesis = cur_partial_hypothesis + [token_idx]\r\n",
        "                new_item = (new_score, new_hypothesis)\r\n",
        "\r\n",
        "                if token_idx == self.eos_token_id or len(new_hypothesis) - initial_length >= max_steps_n:\r\n",
        "                    final_hypotheses.append(new_item)\r\n",
        "                else:\r\n",
        "                    heapq.heappush(partial_hypotheses, new_item)\r\n",
        "\r\n",
        "            if len(partial_hypotheses) > beamsize:\r\n",
        "                partial_hypotheses = heapq.nsmallest(beamsize, partial_hypotheses)\r\n",
        "                heapq.heapify(partial_hypotheses)\r\n",
        "\r\n",
        "        final_scores, final_token_lists = zip(*final_hypotheses)\r\n",
        "        final_texts = self.tokenizer.decode(list(final_token_lists))\r\n",
        "\r\n",
        "        result = list(zip(final_scores, final_texts))\r\n",
        "        result.sort()\r\n",
        "        result = result[:return_hypotheses_n]\r\n",
        "\r\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rU5MrlzEygY"
      },
      "source": [
        "class ProbGenerator:\r\n",
        "    def __init__(self, model, tokenizer, device='cuda', eos_token_id=3, max_steps_n=40, temperature=1.0):\r\n",
        "        self.model = model\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.device = torch.device(device)\r\n",
        "        self.model.to(self.device)\r\n",
        "        self.eos_token_id = eos_token_id\r\n",
        "        self.max_steps_n = max_steps_n\r\n",
        "        self.temperature = temperature\r\n",
        "\r\n",
        "    def __call__(self, seed_text):\r\n",
        "        seed_tokens = self.tokenizer.encode([seed_text])[0]\r\n",
        "        \r\n",
        "        with torch.no_grad():\r\n",
        "            for _ in range(self.max_steps_n):\r\n",
        "                in_batch = torch.tensor(seed_tokens).unsqueeze(0).to(self.device)\r\n",
        "                logits_next = self.model(in_batch)[0, -1]\r\n",
        "                p_next = F.softmax(logits_next / self.temperature, dim=-1).data.cpu().numpy()\r\n",
        "                next_token = np.random.choice(len(tokenizer.vocab()), p=p_next)\r\n",
        "                if next_token == self.eos_token_id:\r\n",
        "                    break\r\n",
        "                seed_tokens.append(next_token)\r\n",
        "\r\n",
        "        return ''.join(self.tokenizer.decode([seed_tokens], ignore_ids=[0,2,3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxTodB_qpyEX"
      },
      "source": [
        "## Loading dataset and splitting it into training and test samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BONUO-4xjwZ"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kboz1dTKxaCh"
      },
      "source": [
        "dataset_filename = \"/content/gdrive/My Drive/ML/datasets/Hermann_Hesse_bibliography_ru.txt\"\r\n",
        "all_chunks = split_into_chunks(dataset_filename, chunk_size=500)\r\n",
        "len(all_chunks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:27:55.954154Z",
          "start_time": "2019-11-05T18:27:55.919185Z"
        },
        "id": "nL1MgC8NpyEY"
      },
      "source": [
        "np.random.shuffle(all_chunks)\n",
        "\n",
        "TRAIN_SPLIT = int(len(all_chunks) * 0.7)\n",
        "train_sample = all_chunks[:TRAIN_SPLIT]\n",
        "val_sample = all_chunks[TRAIN_SPLIT:]\n",
        "\n",
        "print(\"Training sample size:\", len(train_sample))\n",
        "print(\"Validation sample size:\", len(val_sample))\n",
        "\n",
        "# Save train sample in file for further BPE training:\n",
        "TRAIN_SAMPLE_FILENAME = \"/tmp/train_sample.txt\"\n",
        "\n",
        "with open(TRAIN_SAMPLE_FILENAME, 'w') as f:\n",
        "    f.write('\\n'.join(train_sample))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_pCUk_kpyEZ"
      },
      "source": [
        "##  BPE tokenization using [youtokentome library](https://pypi.org/project/youtokentome/):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veO9baR95w4R"
      },
      "source": [
        "NUM_TOKENS_BPE = 1000\r\n",
        "BPE_MODEL_FILENAME = \"/tmp/bpe_model.yttm\"\r\n",
        "yttm.BPE.train(data=TRAIN_SAMPLE_FILENAME, vocab_size=NUM_TOKENS_BPE, model=BPE_MODEL_FILENAME)\r\n",
        "\r\n",
        "tokenizer = yttm.BPE(BPE_MODEL_FILENAME)\r\n",
        "\r\n",
        "train_tokenized_sample = tokenizer.encode(train_sample, bos=True, eos=True)\r\n",
        "val_tokenized_sample = tokenizer.encode(val_sample, bos=True, eos=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFenMS90ghZU"
      },
      "source": [
        "print(train_tokenized_sample[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:27:57.897826Z",
          "start_time": "2019-11-05T18:27:57.874631Z"
        },
        "id": "DYo6GFY3pyEb"
      },
      "source": [
        "print(tokenizer.vocab())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:28:00.401753Z",
          "start_time": "2019-11-05T18:27:59.731680Z"
        },
        "id": "ko2WdTUGpyEc"
      },
      "source": [
        "plt.hist([len(sent) for sent in train_tokenized_sample], bins=30)\n",
        "plt.title('Distribution of tokenized fragment length')\n",
        "plt.yscale('log');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:28:01.153867Z",
          "start_time": "2019-11-05T18:28:00.404320Z"
        },
        "id": "rBRwKqkxpyEc"
      },
      "source": [
        "token_counts = np.bincount([token_id for chunk in val_tokenized_sample for token_id in chunk])\n",
        "\n",
        "plt.hist(token_counts, bins=100)\n",
        "plt.title('Tokens mention distribution')\n",
        "plt.yscale('log');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZRepDx8pyEd"
      },
      "source": [
        "## Creation of [datasets](#scrollTo=OYs7y8NkMDFJ&line=8&uniqifier=1) for PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:28:02.980335Z",
          "start_time": "2019-11-05T18:28:02.938616Z"
        },
        "id": "K73PIdFHpyEd"
      },
      "source": [
        "CHUNK_LENGTH = 200\n",
        "\n",
        "train_dataset = LanguageModelDataset(train_tokenized_sample,\n",
        "                                     chunk_length=CHUNK_LENGTH)\n",
        "val_dataset = LanguageModelDataset(val_tokenized_sample,\n",
        "                                    chunk_length=CHUNK_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEeDgemizvda"
      },
      "source": [
        "## Finding the appropriate batch size for train and validation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8I125zlqxQu"
      },
      "source": [
        "print(f\"divisors of train dataset size ({len(train_dataset)}) are {divisors(len(train_dataset))}\")\r\n",
        "print(f\"divisors of val dataset size ({len(val_dataset)}) are {divisors(len(val_dataset))}\")\r\n",
        "\r\n",
        "batch_size_train = 66\r\n",
        "batch_size_val = 9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-aOq3AKpyEh"
      },
      "source": [
        "## Using PyTorchEncoderLayer and our [LanguageModel class](#scrollTo=QZBmKyALZ5lq&line=1&uniqifier=1) for model creation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:28:10.550078Z",
          "start_time": "2019-11-05T18:28:10.425261Z"
        },
        "id": "KzZcp4K9pyEi"
      },
      "source": [
        "emb_size = 300             # if use BPEmb: SUPPORTED_DIMS = [25, 50, 100, 200, 300]\n",
        "heads_number = 15\n",
        "dim_feedforward = 500\n",
        "layers_number = 6\n",
        "emb_dropout = 0.15\n",
        "layer_dropout = 0.15\n",
        "\n",
        "vectors = BPEmb(language='ru', dim=emb_size,  merge_ops=min([1000, 3000, 5000, 10000, 25000, 50000, 100000, 200000], key=lambda x:abs(x-tokenizer.vocab_size())))\n",
        "emb_weights = vectors[tokenizer.vocab()]\n",
        "\n",
        "TransformerEncoderLayer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=heads_number, dim_feedforward=dim_feedforward, dropout=layer_dropout, activation='gelu')\n",
        "\n",
        "backbone = BatchFirstTransformerEncoder(TransformerEncoderLayer, num_layers=layers_number)\n",
        "\n",
        "torch_transf_model = LanguageModel(vocab_size=tokenizer.vocab_size(), emb_size=emb_size, backbone=backbone, emb_weights=emb_weights, freeze=False, emb_dropout=emb_dropout)\n",
        "\n",
        "print('Number of parameters in model:', get_params_number(torch_transf_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "frTGa-BIEHNv"
      },
      "source": [
        "#@title For loading model\n",
        "emb_size = 300             # if use BPEmb: SUPPORTED_DIMS = [25, 50, 100, 200, 300]\n",
        "heads_number = 15\n",
        "dim_feedforward = 500\n",
        "layers_number = 6\n",
        "emb_dropout = 0.15\n",
        "layer_dropout = 0.15\n",
        "\n",
        "vectors = BPEmb(language='ru', dim=emb_size,  merge_ops=min([1000, 3000, 5000, 10000, 25000, 50000, 100000, 200000], key=lambda x:abs(x-tokenizer.vocab_size())))\n",
        "emb_weights = vectors[tokenizer.vocab()]\n",
        "\n",
        "TransformerEncoderLayer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=heads_number, dim_feedforward=dim_feedforward, dropout=layer_dropout, activation='gelu')\n",
        "\n",
        "backbone = BatchFirstTransformerEncoder(TransformerEncoderLayer, num_layers=layers_number)\n",
        "\n",
        "best_torch_transf_model = LanguageModel(vocab_size=tokenizer.vocab_size(), emb_size=emb_size, backbone=backbone, emb_weights=emb_weights, freeze=False, emb_dropout=emb_dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab9_bB2Quild"
      },
      "source": [
        "## [Training](#scrollTo=jx44W288jjRP&line=1&uniqifier=1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:28:58.797642Z",
          "start_time": "2019-11-05T18:28:34.626744Z"
        },
        "id": "evcA3YG3pyEi"
      },
      "source": [
        "lr_scheduler = lambda optim: \\\n",
        "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=10, factor=0.75, verbose=True)\n",
        "\n",
        "#lr_scheduler = lambda optim: \\\n",
        "#    torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.9)\n",
        "\n",
        "best_val_loss, best_torch_transf_model = train_eval_loop(torch_transf_model,\n",
        "                                                         train_dataset,\n",
        "                                                         val_dataset,\n",
        "                                                         lm_cross_entropy,\n",
        "                                                         lr=5e-4,\n",
        "                                                         epoch_n=3000,\n",
        "                                                         batch_size_train=batch_size_train,\n",
        "                                                         batch_size_val=batch_size_val,\n",
        "                                                         device='cuda',\n",
        "                                                         early_stopping_patience=30,\n",
        "                                                         lr_scheduler_ctor=lr_scheduler,\n",
        "                                                         draw_loss=False,\n",
        "                                                         dataloader_workers_n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CptBPDz8tWxd"
      },
      "source": [
        "MODEL_FILENAME = f\"/tmp/best_torch_transf_model_{best_val_loss}.pth\"\r\n",
        "torch.save(best_torch_transf_model.state_dict(), MODEL_FILENAME)\r\n",
        "files.download(MODEL_FILENAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDOJilyxpyEk"
      },
      "source": [
        "## Text generation using language modeling:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtput-CmpyEk"
      },
      "source": [
        "### Greedy generation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:29:02.366423Z",
          "start_time": "2019-11-05T18:29:02.329495Z"
        },
        "id": "U-TVEKzYpyEl"
      },
      "source": [
        "greedy_generator = GreedyGenerator(best_torch_transf_model, tokenizer)\r\n",
        "max_steps = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:29:03.175509Z",
          "start_time": "2019-11-05T18:29:02.921960Z"
        },
        "id": "AkmVKww6pyEl"
      },
      "source": [
        "print(greedy_generator('Смысл жизни в том,', max_steps_n=max_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:29:03.497702Z",
          "start_time": "2019-11-05T18:29:03.177598Z"
        },
        "id": "jvrdj553pyEl"
      },
      "source": [
        "print(greedy_generator('Наш мир - это', max_steps_n=max_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:29:03.770265Z",
          "start_time": "2019-11-05T18:29:03.500330Z"
        },
        "id": "tDaXIIG0pyEm"
      },
      "source": [
        "print(greedy_generator('В конце концов,', max_steps_n=max_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:29:04.150676Z",
          "start_time": "2019-11-05T18:29:03.773669Z"
        },
        "id": "PxcEJypWpyEm"
      },
      "source": [
        "print(greedy_generator('Я ведь всего только и хотел пытаться жить тем, ', max_steps_n=max_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyEFB9JXpyEm"
      },
      "source": [
        "### Generation using beam search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:29:08.328662Z",
          "start_time": "2019-11-05T18:29:08.294006Z"
        },
        "id": "JL_cnQ-WpyEn"
      },
      "source": [
        "beam_generator = BeamGenerator(best_torch_transf_model, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:29:10.573399Z",
          "start_time": "2019-11-05T18:29:09.653198Z"
        },
        "id": "ub9oEWgSpyEn"
      },
      "source": [
        "%%time\n",
        "\n",
        "beam_gen_variants = beam_generator('Я ведь всего только и хотел пытаться жить тем, ', beamsize=5, return_hypotheses_n=5)\n",
        "\n",
        "for score, pred_txt in beam_gen_variants:\n",
        "    print('****')\n",
        "    print(score)\n",
        "    print(pred_txt)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-05T18:27:05.050342Z",
          "start_time": "2019-11-05T18:27:05.005Z"
        },
        "scrolled": false,
        "id": "8_T_4AqjpyEn"
      },
      "source": [
        "%%time\n",
        "\n",
        "beam_gen_variants = beam_generator('Я ведь всего только и хотел пытаться жить тем, ', beamsize=20, return_hypotheses_n=20)\n",
        "\n",
        "for score, pred_txt in beam_gen_variants:\n",
        "    print('****')\n",
        "    print(score)\n",
        "    print(pred_txt)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY9aZiJPtYoQ"
      },
      "source": [
        "gen = ProbGenerator(best_torch_transf_model, tokenizer, max_steps_n=50, temperature=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iQwET5EI2Zr"
      },
      "source": [
        "print(gen(\"Я ведь всего только и хотел пытаться жить тем, \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNows2lqI7Gh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}